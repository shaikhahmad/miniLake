{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aacb167-452f-46b9-8d68-79ef31a597d0",
   "metadata": {},
   "source": [
    "## Introduction to Spark\n",
    "\n",
    "Industries are using Hadoop extensively to analyze their data sets. The reason is that Hadoop framework is based on a simple programming model (MapReduce), and it enables a computing solution that is scalable, flexible, fault-tolerant and cost effective. Here, the main concern is to maintain speed in processing large datasets in terms of waiting time between queries and waiting time to run the program.\n",
    "\n",
    "Problems of Hadoop: No functionality available to minimize shuffling, minimize storage on hard disk and no efficient scheduling mechanism available - workflow management is weak (Apache oozie is a workflow manager for Hadoop but its technology is inefficient as compared to Spark’s DAG).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893500c9-e4ee-42b5-b158-e34e796f71e5",
   "metadata": {},
   "source": [
    "## Connection to Spark Cluster (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2420701d-8e92-4a9d-b8d6-47b699a63096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d2dd1a-ce0d-41ce-9031-6e6178827e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('jupyter-spark') \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64fbb3dc-5056-4d21-a3df-4325c4ba7982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jovyan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f64cc51b130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if spark is working or not\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3df469e-877a-4f47-95b7-ec8f63dcb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662523b5-c440-4c80-b129-3f064769fe26",
   "metadata": {},
   "source": [
    "## Loading file on HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d4da03c-1c8b-4ed2-88d3-2e31bf0f0545",
   "metadata": {},
   "source": [
    "hey i did not want this but that is how the world works do you know what that means the world is round and turning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7f78a-8122-4d7f-a95f-bdef3217bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hadoop fs -mkdir -p /tmp/spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555dd628-629c-430c-b836-e25dfa50ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/spark/input.txt': File exists\n",
      "Found 1 items\n",
      "-rw-r--r--   2 jovyan supergroup        114 2023-12-23 06:53 /tmp/spark/input.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put /notebook/input.txt /tmp/spark/\n",
    "!hadoop fs -ls /tmp/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbb9de4-7e69-44d1-8f3f-285ce2e1ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('hdfs:///tmp/spark/input.txt')\n",
    "# rdd = spark.read.text('file:///notebook/input.txt')\n",
    "# with open('input.txt', 'r') as reader:\n",
    "#     rdd = sc.parallelize(reader.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96f7719-48fe-48cd-878b-5995b9a2c7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd.foreach(lambda f: print(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65654255-4a7b-4319-899a-281eb9bfcd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey i did not want this but that is how the world works',\n",
       " 'do you know what that means',\n",
       " 'the world is round and turning']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the above function is not working we'll use \n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3da367-de4f-4817-bbd2-50783e633cc4",
   "metadata": {},
   "source": [
    "### Get the number of partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e154cd86-ccdc-485a-a256-01712bc0cf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e455d8a-cd6f-4019-a252-a8d80ea3c5cc",
   "metadata": {},
   "source": [
    "Re-partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5226b400-caa7-469d-8a01-e1a71af461ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reparRDD = rdd.repartition(4)\n",
    "reparRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a650dee8-96bf-4ab3-b054-b0233de3e9a8",
   "metadata": {},
   "source": [
    "## map():\n",
    "\n",
    "The map function iterates over every line in RDD and splits into new RDD. Using map() transformation we take in any function, and that function is applied to every element of RDD.\n",
    "\n",
    "In the map, we have the flexibility that the input and the return type of RDD may differ from each other. For example, we can input RDD type as String, after applying the map() function the return RDD can be Boolean.\n",
    "\n",
    "For example, in RDD {1, 2, 3, 4, 5} if we apply “rdd.map(x=>x+2)” we will get the result as (3, 4, 5, 6, 7).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e993231-fe5f-4e34-8a76-1de6cbf386da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hey i did not want this but that is how the world works', 55),\n",
       " ('do you know what that means', 27),\n",
       " ('the world is round and turning', 30)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapFile = rdd.map(lambda line: (line, len(line)))\n",
    "# mapFile.foreach(lambda f: print(f))\n",
    "mapFile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f4d84-782e-4371-bca8-706b062bd761",
   "metadata": {},
   "source": [
    "## flatMap():\n",
    "\n",
    "With the help of flatMap() function, to each input element, we have many elements in an output RDD. The most simple use of flatMap() is to split each input string into words. Map and flatMap are similar in the way that they take a line from input RDD and apply a function on that line. The key difference between map() and flatMap() is map() returns only one element, while flatMap() can return a list of elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f6e71e0-2869-4920-9c4b-d236f1f72fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey i did not want this but that is how the world works\n",
      "do you know what that means\n",
      "the world is round and turning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'i',\n",
       " 'did',\n",
       " 'not',\n",
       " 'want',\n",
       " 'this',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'how',\n",
       " 'the',\n",
       " 'world',\n",
       " 'works',\n",
       " 'do',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'that',\n",
       " 'means',\n",
       " 'the',\n",
       " 'world',\n",
       " 'is',\n",
       " 'round',\n",
       " 'and',\n",
       " 'turning']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in rdd.collect():\n",
    "    print(line)\n",
    "rdd2 = rdd.flatMap(lambda f: f.split(\" \"))\n",
    "# rdd2.foreach(lambda f: print(f))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b2bfc2-6c33-4374-8c94-50d6e5922566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hey', 1),\n",
       " ('i', 1),\n",
       " ('did', 1),\n",
       " ('not', 1),\n",
       " ('want', 1),\n",
       " ('this', 1),\n",
       " ('but', 1),\n",
       " ('that', 1),\n",
       " ('is', 1),\n",
       " ('how', 1),\n",
       " ('the', 1),\n",
       " ('world', 1),\n",
       " ('works', 1),\n",
       " ('do', 1),\n",
       " ('you', 1),\n",
       " ('know', 1),\n",
       " ('what', 1),\n",
       " ('that', 1),\n",
       " ('means', 1),\n",
       " ('the', 1),\n",
       " ('world', 1),\n",
       " ('is', 1),\n",
       " ('round', 1),\n",
       " ('and', 1),\n",
       " ('turning', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another RDD (rdd3) by applying the map transformation\n",
    "rdd3 = rdd2.map(lambda m: (m, 1))\n",
    "\n",
    "# Use foreach to print each element in the new RDD (rdd3)\n",
    "rdd3.foreach(lambda x: print(x))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2afd30-9416-478d-bb37-6680cc45e207",
   "metadata": {},
   "source": [
    "## Filter transformation:\n",
    "\n",
    "Returns a new RDD, containing only the elements that meet a predicate. For example, suppose RDD contains the first five natural numbers (1, 2, 3, 4, and 5) and the predicate is a check for an even number. The resulting RDD after the filter will contain only the even numbers i.e., 2 and 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3cb6b79-a80b-48eb-b3f0-fc91871bc806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 1), ('that', 1), ('the', 1), ('that', 1), ('the', 1), ('turning', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another RDD (rdd4) by applying the filter transformation\n",
    "rdd4 = rdd3.filter(lambda a: a[0].startswith(\"t\"))\n",
    "\n",
    "# Use foreach to print each element in the filtered RDD (rdd4)\n",
    "# rdd4.foreach(lambda x: print(x))\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9a287-dae6-4d0f-a77f-22b2ebddcb2f",
   "metadata": {},
   "source": [
    "## mapPartitions(func):\n",
    "\n",
    "Converts each partition of the source RDD into many elements of the result (possibly none). In mapPartition(), the map() function is applied on each partition simultaneously. mapPartition() is like a map, but the difference is it runs separately on each partition (block) of the RDD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4a947-d33d-4735-8952-7a9688e33afe",
   "metadata": {},
   "source": [
    "## mapPartitionWithIndex()\n",
    "\n",
    "It is like mapPartition; Besides mapPartition it provides func with an integer value representing the index of the partition, and the map() is applied on partition index wise one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "494ffecd-d952-490c-825b-8b6d444bb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: >>>  ParallelCollectionRDD[14] at readRDDFromFile at PythonRDD.scala:274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['yellow', 'red', 'blue', 'cyan', 'black']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD using parallelize with 3 partitions\n",
    "rdd1 = sc.parallelize([\"yellow\", \"red\", \"blue\", \"cyan\", \"black\"], 3)\n",
    "print(\"rdd: >>> \", rdd1)\n",
    "\n",
    "# Print the elements of the RDD\n",
    "# rdd1.foreach(lambda x: print(x))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6a3a2-977d-440c-830f-01543981d480",
   "metadata": {},
   "source": [
    "## reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a71fa2-4011-4688-a2f2-58fa4ac8dd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 2),\n",
       " ('round', 1),\n",
       " ('turning', 1),\n",
       " ('hey', 1),\n",
       " ('i', 1),\n",
       " ('did', 1),\n",
       " ('want', 1),\n",
       " ('this', 1),\n",
       " ('but', 1),\n",
       " ('works', 1),\n",
       " ('know', 1),\n",
       " ('not', 1),\n",
       " ('that', 2),\n",
       " ('how', 1),\n",
       " ('the', 2),\n",
       " ('world', 2),\n",
       " ('do', 1),\n",
       " ('you', 1),\n",
       " ('what', 1),\n",
       " ('means', 1),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another RDD (rdd5) by applying the reduceByKey transformation\n",
    "rdd5 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use foreach to print each element in the reduced RDD (rdd5)\n",
    "rdd5.foreach(lambda x: print(x))\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f88160-8323-4ebf-9031-eaa9188c99a5",
   "metadata": {},
   "source": [
    "## sortByKey():\n",
    "\n",
    "sorting wrt the count (2nd column) and then the 1st column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a29c9bf9-72b9-46c1-ba36-db1130262422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'round'),\n",
       " (1, 'turning'),\n",
       " (1, 'hey'),\n",
       " (1, 'i'),\n",
       " (1, 'did'),\n",
       " (1, 'want'),\n",
       " (1, 'this'),\n",
       " (1, 'but'),\n",
       " (1, 'works'),\n",
       " (1, 'know'),\n",
       " (1, 'not'),\n",
       " (1, 'how'),\n",
       " (1, 'do'),\n",
       " (1, 'you'),\n",
       " (1, 'what'),\n",
       " (1, 'means'),\n",
       " (1, 'and'),\n",
       " (2, 'that'),\n",
       " (2, 'the'),\n",
       " (2, 'world'),\n",
       " (2, 'is')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create another RDD (rdd6) by applying the map and sortByKey transformations\n",
    "rdd6 = rdd5.map(lambda a: (a[1], a[0])).sortByKey()\n",
    "\n",
    "# Use foreach to print each element in the sorted RDD (rdd6)\n",
    "rdd6.foreach(lambda x: print(x))\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1dd90c-1481-4072-b180-bc8f587aa6cb",
   "metadata": {},
   "source": [
    "## Actions - top:\n",
    "\n",
    "If ordering is present in our RDD, then we can extract top elements from our RDD using top(). Action top() use default ordering of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186b472b-b49c-497c-85c8-9d257ee426d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file from HDFS into an RDD\n",
    "data = spark.read.text(\"hdfs:///tmp/spark/input.txt\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b738cf4-4c23-4b3f-b581-b89cbe7ab311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(value='hey i did not want this but that is how the world works'), 1)\n",
      "(Row(value='do you know what that means'), 1)\n",
      "(Row(value='the world is round and turning'), 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Map each line to a tuple of the line and its length\n",
    "map_file = data.map(lambda line: (line, len(line)))\n",
    "\n",
    "# Get the top 3 elements based on the length of lines\n",
    "res = map_file.top(3, key=lambda x: x[1])\n",
    "\n",
    "# Use foreach to print each element in the result\n",
    "for element in res:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4070678-005a-47da-9fe3-887b37f0f88e",
   "metadata": {},
   "source": [
    "## Action – foreach\n",
    "\n",
    "When we have a situation where we want to apply operation on each element of RDD, but it should not return value to the driver. In this case, foreach() function is useful. For example, inserting a record into the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5ac683-7d09-4818-a76a-d520c9ae1433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'not'),\n",
       " (1, 'how'),\n",
       " (1, 'do'),\n",
       " (1, 'you'),\n",
       " (1, 'what'),\n",
       " (1, 'means'),\n",
       " (1, 'and'),\n",
       " (1, 'round'),\n",
       " (1, 'turning'),\n",
       " (1, 'hey'),\n",
       " (1, 'i'),\n",
       " (1, 'did'),\n",
       " (1, 'want'),\n",
       " (1, 'this'),\n",
       " (1, 'but'),\n",
       " (1, 'works'),\n",
       " (1, 'know'),\n",
       " (2, 'is'),\n",
       " (2, 'that'),\n",
       " (2, 'the'),\n",
       " (2, 'world')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6.foreach(lambda x: print(x)) # might not workrdd6\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c72cb2-b170-47a9-a0ab-5e6c17840d7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'collect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rdd_test \u001b[38;5;241m=\u001b[39m rdd6\u001b[38;5;241m.\u001b[39mforeach(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdfkhdfgkjhdfj\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrdd_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'collect'"
     ]
    }
   ],
   "source": [
    "rdd_test = rdd6.foreach(lambda x: \"sdfkhdfgkjhdfj\" )\n",
    "rdd_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "726fa57b-e3cf-45b1-a8ba-aa92807c8e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Row(value='hey i did not want this but that is how the world works'), 1): 1\n",
      "(Row(value='do you know what that means'), 1): 1\n",
      "(Row(value='the world is round and turning'), 1): 1\n"
     ]
    }
   ],
   "source": [
    "#Count the occurrences of each unique tuple\n",
    "result = map_file.countByValue()\n",
    "\n",
    "# Use foreach to print each unique tuple and its count\n",
    "for (value, count) in result.items():\n",
    "    print(f\"{value}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14835d46-dd09-4193-871d-3d16519147b5",
   "metadata": {},
   "source": [
    "## Action – count\n",
    "\n",
    "count() returns the number of elements in RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5267c54a-8022-45fa-a542-f3a1b67af8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Count : \" + str(rdd6.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82af11-188c-4530-9e5c-834146215f60",
   "metadata": {},
   "source": [
    "## Action – max\n",
    "\n",
    "Returns the maximum value in the RDD – below we are printing max wrt both columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb70f16-6ae5-427e-be9e-8e5dbbf9ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datMax = rdd6.max()\n",
    "print(\"Max Record : {}, {}\".format(datMax[0], datMax[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1ddab-46f9-4164-b6b6-6f915c50dc8d",
   "metadata": {},
   "source": [
    "## Action – reduce\n",
    "\n",
    "The reduce() function takes the two elements as input from the RDD and then produces the output of the same type as that of the input elements. The simple forms of such function are an addition. We can add the elements of RDD, count the number of words. It accepts commutative and associative operations as an argument.\n",
    "\n",
    "We will add the total number of words in the file, and then print the last word – this will add the word count pair by pair in an iterative fashion. So, the final word will be printed in b._2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bf5f6-0850-4a93-8f5e-ef333ac37ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalWordCount = rdd6.reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "print(\"dataReduce Record : {}\".format(totalWordCount[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e6198-fd6e-4d1a-a907-ad7dcfb95a45",
   "metadata": {},
   "source": [
    "## Action – take(n)\n",
    "\n",
    "The action take(n) returns n number of elements from RDD. It tries to cut the number of partition it accesses, so it represents a biased collection. We cannot presume the order of the elements. For example, consider RDD {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “take (4)” will give result { 2, 2, 3, 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cd02d-3286-4a63-bde8-8e4754e4bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = rdd6.take(3)\n",
    "\n",
    "# Use foreach to print each element in the first 3 records\n",
    "for f in data3:\n",
    "    print(\"data3 Key: {}, Value: {}\".format(f[0], f[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12041e-e113-45dd-bc67-484b1cfac880",
   "metadata": {},
   "source": [
    "As you can see above, the selection is not random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5a0fd-4fc1-46e3-8145-2dd9e22c488c",
   "metadata": {},
   "source": [
    "## Action – collect\n",
    "\n",
    "The action collect() is the common and simplest operation that returns our entire RDDs content to driver program. The application of collect() is unit testing where the entire RDD is expected to fit in memory. As a result, it makes easy to compare the result of RDD with the expected result. Action Collect() had a constraint that all the data should fit in the machine, and copies to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbeb8c7-213d-4f42-a6c5-c935b05def4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rdd6.collect()\n",
    "\n",
    "# Use foreach to print each element in the collected list\n",
    "for f in data:\n",
    "    print(\"Key: {}, Value: {}\".format(f[0], f[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83d5f7-6558-40df-bc15-226088296d5a",
   "metadata": {},
   "source": [
    "## union(dataset)\n",
    "\n",
    "Get the elements of both RDDs in new RDD. The key rule of this function is that the two RDDs should be of the same type. For example, the elements of RDD1 are (Spark, Spark, Hadoop, Flink) and that of RDD2 are (Big data, Spark, Flink) so the resultant rdd1.union(rdd2) will have elements (Spark, Spark, Spark, Hadoop, Flink, Flink, Big data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab5fbb-6763-44da-86a5-0c0a24061b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create RDDs\n",
    "rdd1 = sc.parallelize([(1, \"jan\", 2016), (3, \"nov\", 2014), (16, \"feb\", 2014)])\n",
    "rdd2 = sc.parallelize([(5, \"dec\", 2014), (17, \"sep\", 2015)])\n",
    "rdd3 = sc.parallelize([(6, \"dec\", 2011), (16, \"may\", 2015)])\n",
    "\n",
    "# Use union to combine the RDDs\n",
    "rdd_union = rdd1.union(rdd2).union(rdd3)\n",
    "\n",
    "# Use foreach to print each element in the combined RDD\n",
    "rdd_union.foreach(lambda x: print(x)) # might not work\n",
    "rdd_union.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c3003-a76b-4a96-a48c-1bcae6b31930",
   "metadata": {},
   "source": [
    "## intersection(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41149f0-d614-400d-a90d-33d60cb4302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDDs\n",
    "rdd1 = sc.parallelize([(1, \"jan\", 2016), (3, \"nov\", 2014), (16, \"feb\", 2014)])\n",
    "rdd2 = sc.parallelize([(5, \"dec\", 2014), (1, \"jan\", 2016)])\n",
    "\n",
    "# Use intersection to find common elements between rdd1 and rdd2\n",
    "common = rdd1.intersection(rdd2)\n",
    "\n",
    "# Use foreach to print each common element\n",
    "common.foreach(lambda x: print(x))\n",
    "common.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25191820-2c19-4927-a0d9-e51593697d8b",
   "metadata": {},
   "source": [
    "## distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2500953-83e4-44b7-ba21-7f731b470f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd1 = sc.parallelize([(1, \"jan\", 2016), (3, \"nov\", 2014), (16, \"feb\", 2014), (3, \"nov\", 2014)])\n",
    "\n",
    "# Use distinct to obtain distinct elements\n",
    "result = rdd1.distinct()\n",
    "\n",
    "# Collect and print the distinct elements\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde906f3-9b92-4771-af63-3a6bf6fce7c6",
   "metadata": {},
   "source": [
    "## groupByKey():\n",
    "\n",
    "When we use groupByKey() on a dataset of (K, V) pairs, the data is shuffled according to the key value K in another RDD. In this transformation, lots of unnecessary data get to transfer over the network.\n",
    "\n",
    "Spark provides the provision to save data to disk when there is more data shuffled onto a single executor machine than can fit in memory (RDD Caching and Persistence mechanism).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c232b-e098-4e8b-8435-20f99024ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "data = sc.parallelize([('k', 5), ('s', 3), ('s', 4), ('p', 7), ('p', 5), ('t', 8), ('k', 6)], 3)\n",
    "\n",
    "# Use groupByKey to group elements by key\n",
    "group = data.groupByKey().collect()\n",
    "\n",
    "# Use foreach to print each group\n",
    "for key, values in group:\n",
    "    print(\"Key: {}, Values: {}\".format(key, list(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efba0fd-3c53-449b-a915-38797c8aa10e",
   "metadata": {},
   "source": [
    "## reduceByKey():\n",
    "\n",
    "When we use reduceByKey on a dataset (K, V), the pairs on the same machine with the same key are combined, before the data is shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9340f78-c589-429d-8e3b-50c4dd79341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of words\n",
    "words = [\"one\", \"two\", \"two\", \"four\", \"five\", \"six\", \"six\", \"eight\", \"nine\", \"ten\"]\n",
    "\n",
    "# Create an RDD from the array and perform the required transformations\n",
    "data = sc.parallelize(words).map(lambda w: (w, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Use foreach to print each word and its count\n",
    "# data.foreach(lambda x: print(x))\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cc8f8-c60f-4d53-b3e3-ac0f433f3a31",
   "metadata": {},
   "source": [
    "## sortByKey():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244bb966-a00e-4fa8-ba87-d05afebfd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD with tuples\n",
    "data = sc.parallelize([(\"maths\", 52), (\"english\", 75), (\"science\", 82), (\"computer\", 65), (\"maths\", 85)])\n",
    "\n",
    "# Use sortByKey to sort the RDD by the key (the first element in each tuple)\n",
    "sorted_data = data.sortByKey()\n",
    "\n",
    "# Use foreach to print each element in the sorted RDD\n",
    "# sorted_data.foreach(lambda x: print(x))\n",
    "sorted_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cdc30-52b6-47f4-b25e-6fc1d228cb11",
   "metadata": {},
   "source": [
    "## join():\n",
    "\n",
    "The Join is database term. It combines the fields from two table using common values. join() operation in Spark is defined on pair-wise RDD. Pair-wise RDDs are RDD in which each element is in the form of tuples. Where the first element is key and the second element is the value.\n",
    "\n",
    "The boon of using keyed data is that we can combine the data together. The join() operation combines two data sets on the basis of the key.\n",
    "\n",
    "Note – The above code will parallelize the Array of String. It will then map each word with count 1, then reduceByKey will merge the count of values having the similar key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca173b3c-2685-4000-8653-6501625db33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two RDDs\n",
    "data = sc.parallelize([('A', 1), ('b', 2), ('c', 3)])\n",
    "data2 = sc.parallelize([('A', 4), ('A', 6), ('b', 7), ('c', 3), ('c', 8)])\n",
    "\n",
    "# Use join to join the two RDDs based on keys\n",
    "result = data.join(data2)\n",
    "\n",
    "# Collect and print the result\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d06c8-efc6-4ed8-9502-5c521f667541",
   "metadata": {},
   "source": [
    "## coalesce():\n",
    "\n",
    "To avoid full shuffling of data we use coalesce() function. In coalesce() we use existing partition so that less data is shuffled. Using this we can cut the number of partitions. Suppose, we have four nodes, and we want only two nodes. Then the data of extra nodes will be kept onto nodes which we kept.\n",
    "\n",
    "The coalesce will decrease the number of partitions of the source RDD to numPartitions define in coalesce argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc1032-dd74-42cd-a945-5967c47cad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd1 = sc.parallelize([\"jan\", \"feb\", \"mar\", \"april\", \"may\", \"jun\"], 3)\n",
    "\n",
    "# Use coalesce to reduce the number of partitions to 2\n",
    "result = rdd1.coalesce(2)\n",
    "\n",
    "# Use foreach to print each element in the coalesced RDD\n",
    "for x in result.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf2d57-751f-4e6c-88ee-fbcb936b2903",
   "metadata": {},
   "source": [
    "## Actions - Fold:\n",
    "\n",
    "Since RDD’s are partitioned, the fold() function takes full advantage of it by first aggregating elements in each partition and then aggregating results of all partitions to get the final result. The result of this function is the same as this RDD type.\n",
    "\n",
    "fold() is like reduce(). Besides, it takes “zero value” as input, which is used for the initial call on each partition. But, the condition with zero value is that it should be the identity element of that operation. The key difference between fold() and reduce() is that, reduce() throws an exception for empty collection, but fold() is defined for empty collection.\n",
    "\n",
    "For example, zero is an identity for addition; one is identity element for multiplication. The return type of fold() is same as that of the element of RDD we are operating on. For example, rdd.fold(0)((x, y) => x + y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3882b-4344-4236-b492-e29b5e54f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "list_rdd = sc.parallelize([1, 2, 3, 4, 5, 3, 2])\n",
    "\n",
    "# Get the number of partitions\n",
    "print(\"Partitions: \" + str(list_rdd.getNumPartitions()))\n",
    "\n",
    "# Calculate the total sum\n",
    "total_sum = list_rdd.fold(0, lambda acc, ele: acc + ele)\n",
    "print(\"Total: \" + str(total_sum))\n",
    "\n",
    "# Calculate the sum with an initial value of 2\n",
    "total_sum_with_init = list_rdd.fold(2, lambda acc, ele: acc + ele)\n",
    "print(\"Total with init value 2: \" + str(total_sum_with_init))\n",
    "\n",
    "# Find the minimum\n",
    "min_value = list_rdd.fold(0, lambda acc, ele: min(acc, ele))\n",
    "print(\"Min: \" + str(min_value))\n",
    "\n",
    "# Find the maximum\n",
    "max_value = list_rdd.fold(0, lambda acc, ele: max(acc, ele))\n",
    "print(\"Max: \" + str(max_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62474339-4a31-4e49-926a-4aac02536e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "input_rdd = sc.parallelize([(\"Z\", 1), (\"A\", 20), (\"B\", 30), (\"C\", 40), (\"B\", 30), (\"B\", 60)])\n",
    "\n",
    "# Calculate the total sum\n",
    "total_sum = input_rdd.fold((\"\", 0), lambda acc, ele: (\"Total\", acc[1] + ele[1]))\n",
    "print(\"Total: \" + str(total_sum))\n",
    "\n",
    "# Calculate the minimum\n",
    "min_value = input_rdd.fold((\"\", float('inf')), lambda acc, ele: (\"Min\", min(acc[1], ele[1])))\n",
    "print(\"Min: \" + str(min_value))\n",
    "\n",
    "# Calculate the maximum\n",
    "max_value = input_rdd.fold((\"\", float('-inf')), lambda acc, ele: (\"Max\", max(acc[1], ele[1])))\n",
    "print(\"Max: \" + str(max_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52350c-6b2a-4837-ad0d-da3f6f24dfe7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In below additionalMarks is an initial value to be used for each partition in folding. This value will be added to the int value of each record in the source RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d365a2-aac6-4967-9dba-7c34cadf4d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd1 = sc.parallelize([(\"maths\", 10), (\"science\", 20)])\n",
    "\n",
    "# Print the number of partitions\n",
    "print(\"Partitions: \" + str(rdd1.getNumPartitions()))\n",
    "\n",
    "# Print the elements in each partition\n",
    "print(\"Elements in each partition: \" + str(rdd1.glom().collect()))\n",
    "\n",
    "# Define additional marks\n",
    "additional_marks = (\"extra\", 1)\n",
    "\n",
    "# Calculate the sum of marks including additional marks\n",
    "sum_result = rdd1.fold(additional_marks, lambda acc, marks: (\"total\", acc[1] + marks[1]))\n",
    "\n",
    "# Print the result\n",
    "print(sum_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c554e-2842-4103-9fbc-ec988ac2133f",
   "metadata": {},
   "source": [
    "*In above, 1 is added to both 10 and 20 to yield 11 and 21. 1 is also added for 6 partitions which are empty. This gives total: 11+21+6=38. Then, 1 is again added to the final aggregation of the partitions to yield 39.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35882bd-e6ca-4087-be9e-68ee630e023d",
   "metadata": {},
   "source": [
    "## CountByValue()\n",
    "\n",
    "It returns the count of each unique value in an RDD as a local Map (as a Map to driver program) (value, countofvalues) pair\n",
    "\n",
    "Care must be taken to use this API since it returns the value to driver program so it’s suitable only for small values.\n",
    "\n",
    "For example, RDD has values {1, 2, 2, 3, 4, 5, 5, 6} in this RDD “rdd.countByValue()” will give the result {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467d27-c985-4c60-813a-5549cb586f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd1 = sc.parallelize([(\"HR\", 5), (\"RD\", 4), (\"ADMIN\", 5), (\"SALES\", 4), (\"SER\", 6), (\"MAN\", 8)])\n",
    "\n",
    "# Use countByValue to get the count of each unique value\n",
    "result = rdd1.countByValue()\n",
    "\n",
    "# Print the result\n",
    "for key, count in result.items():\n",
    "    print(\"{}: {}\".format(key, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076446b-e1b3-43c1-9346-a24a3d7f3b8e",
   "metadata": {},
   "source": [
    "## Aggregate()\n",
    "\n",
    "Since RDD’s are partitioned, the aggregate takes full advantage of it by first aggregating elements in each partition and then aggregating results of all partition to get the final result, and the result could be any type than the type of your RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ec87b-cd05-44f6-b449-f8751d779947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "list_rdd = sc.parallelize([1, 2, 3, 4, 5, 3, 2])\n",
    "\n",
    "# Define the zero value and two functions for aggregation\n",
    "zero_value = 0\n",
    "param0 = lambda acc, v: acc + v\n",
    "param1 = lambda acc1, acc2: acc1 + acc2\n",
    "\n",
    "# Use aggregate to calculate the sum of elements\n",
    "result = list_rdd.aggregate(zero_value, param0, param1)\n",
    "\n",
    "# Print the result\n",
    "print(\"output 1 =>\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5091e-6c8b-4917-bd31-5d53d9859602",
   "metadata": {},
   "source": [
    "## Closing Spark Session <font color='red'>Important!</font> foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10b392-2c28-4ce8-b645-5748f0438b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stoping Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e8046-9e4a-4db1-90a7-ae63d3f7aee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
